## Simulationslauf vom: 2025-11-10 21:05:33

### Globale Parameter
- **Begegnungsschema:** `SpatialGridScheme`
- **Gittergröße:** `(10, 20)`
- **Anzahl Matches/Duelle:** `60000`
- **Runden pro Episode:** `200`
- **Zufalls-Seed:** `8`

### Agenten-Population
- **QL:** `200`
- **Gesamt:** `200` Agenten

### Lern-Hyperparameter
- **alpha:** `0.05`
- **gamma:** `0.95`
- **epsilon:** `1.0`
- **epsilon_decay:** `0.9995`
- **min_epsilon:** `0.01`

---

### Finale Ergebnisse (des obigen Laufs)

**Globale Metriken (Gesamtsystem):**
- **System-Effizienz (% des Max):** `46.92%`
- **Durchschnittliche Kooperationsrate:** `15.94%`

**Metriken pro Agententyp (Durchschnitt über den gesamten Zeitraum):**
- **Avg. Reward/Match:**
  - `QLearningAgent`: 281.43
- **Avg. Kooperationsrate:**
  - `QLearningAgent`: 15.94%

**Finale Cluster-Analyse:**
- **Anzahl gefundener Cluster:** `1`
- **Gesamtfläche pro Strategietyp:**
  - `Defector`: 85.0%
  - `Cautious Coop.`: 7.0%
  - `Polarized`: 4.0%
  - `TFT-like`: 2.5%
  - `Cooperator (ALLC)`: 1.0%
  - `WSLS`: 0.5%
- **Anzahl Cluster pro Strategietyp:**
  - `Defector`: 1

---

## Simulationslauf vom: 2025-11-10 21:12:56

### Globale Parameter
- **Begegnungsschema:** `SpatialGridScheme`
- **Gittergröße:** `(10, 20)`
- **Anzahl Matches/Duelle:** `60000`
- **Runden pro Episode:** `200`
- **Zufalls-Seed:** `9`

### Agenten-Population
- **QL:** `200`
- **Gesamt:** `200` Agenten

### Lern-Hyperparameter
- **alpha:** `0.05`
- **gamma:** `0.95`
- **epsilon:** `1.0`
- **epsilon_decay:** `0.9995`
- **min_epsilon:** `0.01`

---

